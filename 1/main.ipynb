{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yWsQwCLoGk_"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14809,
     "status": "ok",
     "timestamp": 1742951520780,
     "user": {
      "displayName": "Seanmamasde",
      "userId": "11889259840918057157"
     },
     "user_tz": -480
    },
    "id": "M2n0Z7D2mm-9",
    "outputId": "ac8b1356-2c5c-4d3a-dd85-c736de71011a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 142442,
     "status": "ok",
     "timestamp": 1742951663224,
     "user": {
      "displayName": "Seanmamasde",
      "userId": "11889259840918057157"
     },
     "user_tz": -480
    },
    "id": "7LKeKGfxoJ90"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/dataset\n",
    "!cp '/content/drive/MyDrive/Colab Notebooks/computer_vision/1/hw1-data.tar.gz' '/content/dataset'\n",
    "!tar -xf '/content/dataset/hw1-data.tar.gz' -C '/content/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1742951663242,
     "user": {
      "displayName": "Seanmamasde",
      "userId": "11889259840918057157"
     },
     "user_tz": -480
    },
    "id": "Xi9oTk70xAZD",
    "outputId": "ed8005fb-a796-4139-9557-012eb2173b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/computer_vision/1\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/drive/My Drive/Colab Notebooks/computer_vision/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9290,
     "status": "ok",
     "timestamp": 1742951672534,
     "user": {
      "displayName": "Seanmamasde",
      "userId": "11889259840918057157"
     },
     "user_tz": -480
    },
    "id": "Gx_R5Krd2pVH"
   },
   "outputs": [],
   "source": [
    "!apt install htop nvtop 1>/dev/null 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KQZpUIh0OZ3"
   },
   "source": [
    "# `regnety_160`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257,
     "referenced_widgets": [
      "3f9d9cb7345d44c489baa79c86f41c43",
      "9344cbef06d741d08908e4eafe63bc06",
      "f9c075b8840a4f44bc02efe8f930f280",
      "a6f3392bca084170abaf6074f56ef91c",
      "eb7fa1fa3df7445f9062bfe3db66f235",
      "f6e3bcdf64a8456ba01bcd95b96aa3eb",
      "5ce802315d9f4a0ebb6e92209a289c10",
      "38067e201cf542b19bc37710ca06cd16",
      "7fb1886dcd634f2eaa8444789c9e934a",
      "4800c5ab958340f89159b366c355c842",
      "4433dfcdaa3f49d1979cbd73d32fdc50"
     ]
    },
    "executionInfo": {
     "elapsed": 810017,
     "status": "ok",
     "timestamp": 1742966562123,
     "user": {
      "displayName": "Seanmamasde",
      "userId": "11889259840918057157"
     },
     "user_tz": -480
    },
    "id": "PRQzUFxh0P-U",
    "outputId": "afacc17d-fed6-4aba-f9a7-206e6773f3a9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9d9cb7345d44c489baa79c86f41c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9746\n",
      "Epoch 2/10, Loss: 0.3943\n",
      "Epoch 3/10, Loss: 0.3108\n",
      "Epoch 4/10, Loss: 0.2408\n",
      "Epoch 5/10, Loss: 0.2242\n",
      "Epoch 6/10, Loss: 0.1974\n",
      "Epoch 7/10, Loss: 0.1724\n",
      "Epoch 8/10, Loss: 0.1514\n",
      "Epoch 9/10, Loss: 0.1501\n",
      "Epoch 10/10, Loss: 0.1408\n",
      "Saved predictions to prediction.csv\n",
      "Created zip file pred.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import timm\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------\n",
    "# Setup directories and transforms\n",
    "# ----------------------------\n",
    "data_dir = \"/content/dataset/data\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# Define transforms (modify as needed)\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Load training and validation data\n",
    "# ----------------------------\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)\n",
    "\n",
    "# Reverse the mapping so that index -> class label\n",
    "idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# ----------------------------\n",
    "# Create and fine-tune the model\n",
    "# ----------------------------\n",
    "num_classes = len(train_dataset.classes)  # should be 100\n",
    "model = timm.create_model(\"regnety_160\", pretrained=True, num_classes=num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10  # adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6999,
     "status": "ok",
     "timestamp": 1742966738902,
     "user": {
      "displayName": "Seanmamasde",
      "userId": "11889259840918057157"
     },
     "user_tz": -480
    },
    "id": "CeIuX9u56ao0",
    "outputId": "79f6dcdb-8b8e-4667-b71a-821e61fa3ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to prediction.csv\n",
      "Created zip file pred.zip\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Define a custom test dataset\n",
    "# ----------------------------\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_dir, transform=None):\n",
    "        self.test_dir = test_dir\n",
    "        self.transform = transform\n",
    "        self.images = [fname for fname in os.listdir(test_dir) if fname.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.test_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "\n",
    "test_dataset = TestDataset(test_dir, transform=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# ----------------------------\n",
    "# Inference on the test set\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, names in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        preds = preds.cpu().numpy()\n",
    "        for name, pred in zip(names, preds):\n",
    "            # Map numeric prediction back to the original class label using our reversed mapping.\n",
    "            label = idx_to_class[pred]\n",
    "            predictions.append((name.split(\".\")[0], label))\n",
    "\n",
    "# ----------------------------\n",
    "# Save predictions: remove existing prediction.csv and pred.zip if they exist.\n",
    "# ----------------------------\n",
    "csv_filename = \"prediction.csv\"\n",
    "zip_filename = \"pred.zip\"\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    os.remove(csv_filename)\n",
    "if os.path.exists(zip_filename):\n",
    "    os.remove(zip_filename)\n",
    "\n",
    "# Save predictions to CSV in the required format: image_name,pred_label\n",
    "df = pd.DataFrame(predictions, columns=[\"image_name\", \"pred_label\"])\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Saved predictions to {csv_filename}\")\n",
    "\n",
    "# Create a zip file containing the prediction.csv file\n",
    "with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(csv_filename)\n",
    "print(f\"Created zip file {zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ8YtBpF7y09"
   },
   "source": [
    "# `seresnext101_64x4d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3558695,
     "status": "ok",
     "timestamp": 1742988080742,
     "user": {
      "displayName": "Seanmamasde",
      "userId": "11889259840918057157"
     },
     "user_tz": -480
    },
    "id": "CD5ZbOyz7sPD",
    "outputId": "2b2fb984-2df8-4956-b1bf-35b11905ccd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/10\n",
      "  Epoch 1/20, Loss: 1.2066\n",
      "    Validation Accuracy: 0.7467\n",
      "    Model improved. Saved model to ./saved_models/model1_1.pth\n",
      "  Epoch 2/20, Loss: 0.2785\n",
      "    Validation Accuracy: 0.8167\n",
      "    Model improved. Saved model to ./saved_models/model1_2.pth\n",
      "  Epoch 3/20, Loss: 0.1526\n",
      "    Validation Accuracy: 0.8267\n",
      "    Model improved. Saved model to ./saved_models/model1_3.pth\n",
      "  Epoch 4/20, Loss: 0.1111\n",
      "    Validation Accuracy: 0.7900\n",
      "  Epoch 5/20, Loss: 0.0950\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 6/20, Loss: 0.0767\n",
      "    Validation Accuracy: 0.8300\n",
      "    Model improved. Saved model to ./saved_models/model1_6.pth\n",
      "  Epoch 7/20, Loss: 0.0757\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 8/20, Loss: 0.0671\n",
      "    Validation Accuracy: 0.7967\n",
      "  Epoch 9/20, Loss: 0.0464\n",
      "    Validation Accuracy: 0.8033\n",
      "  Epoch 10/20, Loss: 0.0649\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 11/20, Loss: 0.0509\n",
      "    Validation Accuracy: 0.8367\n",
      "    Model improved. Saved model to ./saved_models/model1_11.pth\n",
      "  Epoch 12/20, Loss: 0.0574\n",
      "    Validation Accuracy: 0.8100\n",
      "  Epoch 13/20, Loss: 0.0429\n",
      "    Validation Accuracy: 0.8567\n",
      "    Model improved. Saved model to ./saved_models/model1_13.pth\n",
      "  Epoch 14/20, Loss: 0.0423\n",
      "    Validation Accuracy: 0.8333\n",
      "  Epoch 15/20, Loss: 0.0517\n",
      "    Validation Accuracy: 0.8400\n",
      "  Epoch 16/20, Loss: 0.0499\n",
      "    Validation Accuracy: 0.8100\n",
      "  Epoch 17/20, Loss: 0.0552\n",
      "    Validation Accuracy: 0.8033\n",
      "  Epoch 18/20, Loss: 0.0363\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 19/20, Loss: 0.0400\n",
      "    Validation Accuracy: 0.8100\n",
      "  Epoch 20/20, Loss: 0.0454\n",
      "    Validation Accuracy: 0.8333\n",
      "Finished training model 1\n",
      "\n",
      "Training model 2/10\n",
      "  Epoch 1/20, Loss: 1.2263\n",
      "    Validation Accuracy: 0.7967\n",
      "    Model improved. Saved model to ./saved_models/model2_1.pth\n",
      "  Epoch 2/20, Loss: 0.2674\n",
      "    Validation Accuracy: 0.8100\n",
      "    Model improved. Saved model to ./saved_models/model2_2.pth\n",
      "  Epoch 3/20, Loss: 0.1528\n",
      "    Validation Accuracy: 0.8300\n",
      "    Model improved. Saved model to ./saved_models/model2_3.pth\n",
      "  Epoch 4/20, Loss: 0.1102\n",
      "    Validation Accuracy: 0.8100\n",
      "  Epoch 5/20, Loss: 0.0740\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 6/20, Loss: 0.0861\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 7/20, Loss: 0.0732\n",
      "    Validation Accuracy: 0.8300\n",
      "  Epoch 8/20, Loss: 0.0740\n",
      "    Validation Accuracy: 0.8400\n",
      "    Model improved. Saved model to ./saved_models/model2_8.pth\n",
      "  Epoch 9/20, Loss: 0.0588\n",
      "    Validation Accuracy: 0.8400\n",
      "  Epoch 10/20, Loss: 0.0557\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 11/20, Loss: 0.0479\n",
      "    Validation Accuracy: 0.8400\n",
      "  Epoch 12/20, Loss: 0.0550\n",
      "    Validation Accuracy: 0.8367\n",
      "  Epoch 13/20, Loss: 0.0618\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 14/20, Loss: 0.0483\n",
      "    Validation Accuracy: 0.8600\n",
      "    Model improved. Saved model to ./saved_models/model2_14.pth\n",
      "  Epoch 15/20, Loss: 0.0416\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 16/20, Loss: 0.0476\n",
      "    Validation Accuracy: 0.8400\n",
      "  Epoch 17/20, Loss: 0.0375\n",
      "    Validation Accuracy: 0.8100\n",
      "  Epoch 18/20, Loss: 0.0616\n",
      "    Validation Accuracy: 0.8433\n",
      "  Epoch 19/20, Loss: 0.0336\n",
      "    Validation Accuracy: 0.8333\n",
      "  Epoch 20/20, Loss: 0.0303\n",
      "    Validation Accuracy: 0.8267\n",
      "Finished training model 2\n",
      "\n",
      "Training model 3/10\n",
      "  Epoch 1/20, Loss: 1.2285\n",
      "    Validation Accuracy: 0.7600\n",
      "    Model improved. Saved model to ./saved_models/model3_1.pth\n",
      "  Epoch 2/20, Loss: 0.2699\n",
      "    Validation Accuracy: 0.7833\n",
      "    Model improved. Saved model to ./saved_models/model3_2.pth\n",
      "  Epoch 3/20, Loss: 0.1517\n",
      "    Validation Accuracy: 0.8000\n",
      "    Model improved. Saved model to ./saved_models/model3_3.pth\n",
      "  Epoch 4/20, Loss: 0.1134\n",
      "    Validation Accuracy: 0.8133\n",
      "    Model improved. Saved model to ./saved_models/model3_4.pth\n",
      "  Epoch 5/20, Loss: 0.0894\n",
      "    Validation Accuracy: 0.7900\n",
      "  Epoch 6/20, Loss: 0.0686\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 7/20, Loss: 0.0704\n",
      "    Validation Accuracy: 0.8167\n",
      "    Model improved. Saved model to ./saved_models/model3_7.pth\n",
      "  Epoch 8/20, Loss: 0.0732\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 9/20, Loss: 0.0694\n",
      "    Validation Accuracy: 0.8400\n",
      "    Model improved. Saved model to ./saved_models/model3_9.pth\n",
      "  Epoch 10/20, Loss: 0.0724\n",
      "    Validation Accuracy: 0.8400\n",
      "  Epoch 11/20, Loss: 0.0641\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 12/20, Loss: 0.0606\n",
      "    Validation Accuracy: 0.8467\n",
      "    Model improved. Saved model to ./saved_models/model3_12.pth\n",
      "  Epoch 13/20, Loss: 0.0409\n",
      "    Validation Accuracy: 0.8367\n",
      "  Epoch 14/20, Loss: 0.0505\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 15/20, Loss: 0.0553\n",
      "    Validation Accuracy: 0.7933\n",
      "  Epoch 16/20, Loss: 0.0494\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 17/20, Loss: 0.0401\n",
      "    Validation Accuracy: 0.8333\n",
      "  Epoch 18/20, Loss: 0.0417\n",
      "    Validation Accuracy: 0.8300\n",
      "  Epoch 19/20, Loss: 0.0332\n",
      "    Validation Accuracy: 0.8333\n",
      "  Epoch 20/20, Loss: 0.0401\n",
      "    Validation Accuracy: 0.8267\n",
      "Finished training model 3\n",
      "\n",
      "Training model 4/10\n",
      "  Epoch 1/20, Loss: 1.2212\n",
      "    Validation Accuracy: 0.7667\n",
      "    Model improved. Saved model to ./saved_models/model4_1.pth\n",
      "  Epoch 2/20, Loss: 0.2645\n",
      "    Validation Accuracy: 0.8133\n",
      "    Model improved. Saved model to ./saved_models/model4_2.pth\n",
      "  Epoch 3/20, Loss: 0.1415\n",
      "    Validation Accuracy: 0.8067\n",
      "  Epoch 4/20, Loss: 0.1108\n",
      "    Validation Accuracy: 0.8300\n",
      "    Model improved. Saved model to ./saved_models/model4_4.pth\n",
      "  Epoch 5/20, Loss: 0.0949\n",
      "    Validation Accuracy: 0.8033\n",
      "  Epoch 6/20, Loss: 0.0816\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 7/20, Loss: 0.0755\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 8/20, Loss: 0.0603\n",
      "    Validation Accuracy: 0.8000\n",
      "  Epoch 9/20, Loss: 0.0562\n",
      "    Validation Accuracy: 0.8300\n",
      "  Epoch 10/20, Loss: 0.0725\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 11/20, Loss: 0.0569\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 12/20, Loss: 0.0578\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 13/20, Loss: 0.0497\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 14/20, Loss: 0.0352\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 15/20, Loss: 0.0408\n",
      "    Validation Accuracy: 0.8100\n",
      "  Epoch 16/20, Loss: 0.0380\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 17/20, Loss: 0.0438\n",
      "    Validation Accuracy: 0.8100\n",
      "  Epoch 18/20, Loss: 0.0390\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 19/20, Loss: 0.0549\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 20/20, Loss: 0.0547\n",
      "    Validation Accuracy: 0.7933\n",
      "Finished training model 4\n",
      "\n",
      "Training model 5/10\n",
      "  Epoch 1/20, Loss: 1.2104\n",
      "    Validation Accuracy: 0.7367\n",
      "    Model improved. Saved model to ./saved_models/model5_1.pth\n",
      "  Epoch 2/20, Loss: 0.2773\n",
      "    Validation Accuracy: 0.8000\n",
      "    Model improved. Saved model to ./saved_models/model5_2.pth\n",
      "  Epoch 3/20, Loss: 0.1543\n",
      "    Validation Accuracy: 0.8233\n",
      "    Model improved. Saved model to ./saved_models/model5_3.pth\n",
      "  Epoch 4/20, Loss: 0.1046\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 5/20, Loss: 0.0906\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 6/20, Loss: 0.0828\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 7/20, Loss: 0.0793\n",
      "    Validation Accuracy: 0.8067\n",
      "  Epoch 8/20, Loss: 0.0557\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 9/20, Loss: 0.0624\n",
      "    Validation Accuracy: 0.8067\n",
      "  Epoch 10/20, Loss: 0.0573\n",
      "    Validation Accuracy: 0.7900\n",
      "  Epoch 11/20, Loss: 0.0576\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 12/20, Loss: 0.0588\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 13/20, Loss: 0.0496\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 14/20, Loss: 0.0533\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 15/20, Loss: 0.0546\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 16/20, Loss: 0.0419\n",
      "    Validation Accuracy: 0.7933\n",
      "  Epoch 17/20, Loss: 0.0464\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 18/20, Loss: 0.0379\n",
      "    Validation Accuracy: 0.8300\n",
      "    Model improved. Saved model to ./saved_models/model5_18.pth\n",
      "  Epoch 19/20, Loss: 0.0349\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 20/20, Loss: 0.0360\n",
      "    Validation Accuracy: 0.8367\n",
      "    Model improved. Saved model to ./saved_models/model5_20.pth\n",
      "Finished training model 5\n",
      "\n",
      "Training model 6/10\n",
      "  Epoch 1/20, Loss: 1.2019\n",
      "    Validation Accuracy: 0.7800\n",
      "    Model improved. Saved model to ./saved_models/model6_1.pth\n",
      "  Epoch 2/20, Loss: 0.2789\n",
      "    Validation Accuracy: 0.8000\n",
      "    Model improved. Saved model to ./saved_models/model6_2.pth\n",
      "  Epoch 3/20, Loss: 0.1555\n",
      "    Validation Accuracy: 0.8100\n",
      "    Model improved. Saved model to ./saved_models/model6_3.pth\n",
      "  Epoch 4/20, Loss: 0.1132\n",
      "    Validation Accuracy: 0.8267\n",
      "    Model improved. Saved model to ./saved_models/model6_4.pth\n",
      "  Epoch 5/20, Loss: 0.1037\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 6/20, Loss: 0.0866\n",
      "    Validation Accuracy: 0.7900\n",
      "  Epoch 7/20, Loss: 0.0665\n",
      "    Validation Accuracy: 0.8600\n",
      "    Model improved. Saved model to ./saved_models/model6_7.pth\n",
      "  Epoch 8/20, Loss: 0.0630\n",
      "    Validation Accuracy: 0.8433\n",
      "  Epoch 9/20, Loss: 0.0677\n",
      "    Validation Accuracy: 0.8467\n",
      "  Epoch 10/20, Loss: 0.0586\n",
      "    Validation Accuracy: 0.8300\n",
      "  Epoch 11/20, Loss: 0.0557\n",
      "    Validation Accuracy: 0.8300\n",
      "  Epoch 12/20, Loss: 0.0544\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 13/20, Loss: 0.0564\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 14/20, Loss: 0.0447\n",
      "    Validation Accuracy: 0.8100\n",
      "  Epoch 15/20, Loss: 0.0518\n",
      "    Validation Accuracy: 0.8033\n",
      "  Epoch 16/20, Loss: 0.0540\n",
      "    Validation Accuracy: 0.8000\n",
      "  Epoch 17/20, Loss: 0.0460\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 18/20, Loss: 0.0424\n",
      "    Validation Accuracy: 0.8367\n",
      "  Epoch 19/20, Loss: 0.0344\n",
      "    Validation Accuracy: 0.8433\n",
      "  Epoch 20/20, Loss: 0.0572\n",
      "    Validation Accuracy: 0.8067\n",
      "Finished training model 6\n",
      "\n",
      "Training model 7/10\n",
      "  Epoch 1/20, Loss: 1.2022\n",
      "    Validation Accuracy: 0.7500\n",
      "    Model improved. Saved model to ./saved_models/model7_1.pth\n",
      "  Epoch 2/20, Loss: 0.2611\n",
      "    Validation Accuracy: 0.8100\n",
      "    Model improved. Saved model to ./saved_models/model7_2.pth\n",
      "  Epoch 3/20, Loss: 0.1500\n",
      "    Validation Accuracy: 0.8000\n",
      "  Epoch 4/20, Loss: 0.1094\n",
      "    Validation Accuracy: 0.8000\n",
      "  Epoch 5/20, Loss: 0.0923\n",
      "    Validation Accuracy: 0.8267\n",
      "    Model improved. Saved model to ./saved_models/model7_5.pth\n",
      "  Epoch 6/20, Loss: 0.0746\n",
      "    Validation Accuracy: 0.8400\n",
      "    Model improved. Saved model to ./saved_models/model7_6.pth\n",
      "  Epoch 7/20, Loss: 0.0701\n",
      "    Validation Accuracy: 0.8467\n",
      "    Model improved. Saved model to ./saved_models/model7_7.pth\n",
      "  Epoch 8/20, Loss: 0.0619\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 9/20, Loss: 0.0655\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 10/20, Loss: 0.0669\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 11/20, Loss: 0.0563\n",
      "    Validation Accuracy: 0.8500\n",
      "    Model improved. Saved model to ./saved_models/model7_11.pth\n",
      "  Epoch 12/20, Loss: 0.0463\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 13/20, Loss: 0.0475\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 14/20, Loss: 0.0468\n",
      "    Validation Accuracy: 0.8433\n",
      "  Epoch 15/20, Loss: 0.0400\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 16/20, Loss: 0.0544\n",
      "    Validation Accuracy: 0.8067\n",
      "  Epoch 17/20, Loss: 0.0539\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 18/20, Loss: 0.0371\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 19/20, Loss: 0.0383\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 20/20, Loss: 0.0465\n",
      "    Validation Accuracy: 0.8233\n",
      "Finished training model 7\n",
      "\n",
      "Training model 8/10\n",
      "  Epoch 1/20, Loss: 1.1885\n",
      "    Validation Accuracy: 0.7467\n",
      "    Model improved. Saved model to ./saved_models/model8_1.pth\n",
      "  Epoch 2/20, Loss: 0.2703\n",
      "    Validation Accuracy: 0.7800\n",
      "    Model improved. Saved model to ./saved_models/model8_2.pth\n",
      "  Epoch 3/20, Loss: 0.1451\n",
      "    Validation Accuracy: 0.8133\n",
      "    Model improved. Saved model to ./saved_models/model8_3.pth\n",
      "  Epoch 4/20, Loss: 0.1145\n",
      "    Validation Accuracy: 0.8367\n",
      "    Model improved. Saved model to ./saved_models/model8_4.pth\n",
      "  Epoch 5/20, Loss: 0.0885\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 6/20, Loss: 0.0870\n",
      "    Validation Accuracy: 0.8300\n",
      "  Epoch 7/20, Loss: 0.0784\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 8/20, Loss: 0.0693\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 9/20, Loss: 0.0491\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 10/20, Loss: 0.0545\n",
      "    Validation Accuracy: 0.8433\n",
      "    Model improved. Saved model to ./saved_models/model8_10.pth\n",
      "  Epoch 11/20, Loss: 0.0719\n",
      "    Validation Accuracy: 0.8433\n",
      "  Epoch 12/20, Loss: 0.0571\n",
      "    Validation Accuracy: 0.8400\n",
      "  Epoch 13/20, Loss: 0.0557\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 14/20, Loss: 0.0547\n",
      "    Validation Accuracy: 0.8333\n",
      "  Epoch 15/20, Loss: 0.0637\n",
      "    Validation Accuracy: 0.8467\n",
      "    Model improved. Saved model to ./saved_models/model8_15.pth\n",
      "  Epoch 16/20, Loss: 0.0501\n",
      "    Validation Accuracy: 0.8400\n",
      "  Epoch 17/20, Loss: 0.0382\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 18/20, Loss: 0.0439\n",
      "    Validation Accuracy: 0.8567\n",
      "    Model improved. Saved model to ./saved_models/model8_18.pth\n",
      "  Epoch 19/20, Loss: 0.0306\n",
      "    Validation Accuracy: 0.8300\n",
      "  Epoch 20/20, Loss: 0.0299\n",
      "    Validation Accuracy: 0.8367\n",
      "Finished training model 8\n",
      "\n",
      "Training model 9/10\n",
      "  Epoch 1/20, Loss: 1.2050\n",
      "    Validation Accuracy: 0.7600\n",
      "    Model improved. Saved model to ./saved_models/model9_1.pth\n",
      "  Epoch 2/20, Loss: 0.2714\n",
      "    Validation Accuracy: 0.7833\n",
      "    Model improved. Saved model to ./saved_models/model9_2.pth\n",
      "  Epoch 3/20, Loss: 0.1481\n",
      "    Validation Accuracy: 0.8067\n",
      "    Model improved. Saved model to ./saved_models/model9_3.pth\n",
      "  Epoch 4/20, Loss: 0.1135\n",
      "    Validation Accuracy: 0.8033\n",
      "  Epoch 5/20, Loss: 0.0870\n",
      "    Validation Accuracy: 0.8500\n",
      "    Model improved. Saved model to ./saved_models/model9_5.pth\n",
      "  Epoch 6/20, Loss: 0.0898\n",
      "    Validation Accuracy: 0.8533\n",
      "    Model improved. Saved model to ./saved_models/model9_6.pth\n",
      "  Epoch 7/20, Loss: 0.0712\n",
      "    Validation Accuracy: 0.8100\n",
      "  Epoch 8/20, Loss: 0.0827\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 9/20, Loss: 0.0646\n",
      "    Validation Accuracy: 0.8433\n",
      "  Epoch 10/20, Loss: 0.0673\n",
      "    Validation Accuracy: 0.8367\n",
      "  Epoch 11/20, Loss: 0.0527\n",
      "    Validation Accuracy: 0.8500\n",
      "  Epoch 12/20, Loss: 0.0540\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 13/20, Loss: 0.0349\n",
      "    Validation Accuracy: 0.8267\n",
      "  Epoch 14/20, Loss: 0.0475\n",
      "    Validation Accuracy: 0.8300\n",
      "  Epoch 15/20, Loss: 0.0553\n",
      "    Validation Accuracy: 0.8367\n",
      "  Epoch 16/20, Loss: 0.0411\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 17/20, Loss: 0.0357\n",
      "    Validation Accuracy: 0.8367\n",
      "  Epoch 18/20, Loss: 0.0289\n",
      "    Validation Accuracy: 0.8367\n",
      "  Epoch 19/20, Loss: 0.0594\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 20/20, Loss: 0.0426\n",
      "    Validation Accuracy: 0.8367\n",
      "Finished training model 9\n",
      "\n",
      "Training model 10/10\n",
      "  Epoch 1/20, Loss: 1.1802\n",
      "    Validation Accuracy: 0.7567\n",
      "    Model improved. Saved model to ./saved_models/model10_1.pth\n",
      "  Epoch 2/20, Loss: 0.2646\n",
      "    Validation Accuracy: 0.8000\n",
      "    Model improved. Saved model to ./saved_models/model10_2.pth\n",
      "  Epoch 3/20, Loss: 0.1400\n",
      "    Validation Accuracy: 0.8067\n",
      "    Model improved. Saved model to ./saved_models/model10_3.pth\n",
      "  Epoch 4/20, Loss: 0.1009\n",
      "    Validation Accuracy: 0.8167\n",
      "    Model improved. Saved model to ./saved_models/model10_4.pth\n",
      "  Epoch 5/20, Loss: 0.1031\n",
      "    Validation Accuracy: 0.8233\n",
      "    Model improved. Saved model to ./saved_models/model10_5.pth\n",
      "  Epoch 6/20, Loss: 0.0791\n",
      "    Validation Accuracy: 0.7933\n",
      "  Epoch 7/20, Loss: 0.0775\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 8/20, Loss: 0.0618\n",
      "    Validation Accuracy: 0.8133\n",
      "  Epoch 9/20, Loss: 0.0524\n",
      "    Validation Accuracy: 0.8000\n",
      "  Epoch 10/20, Loss: 0.0615\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 11/20, Loss: 0.0650\n",
      "    Validation Accuracy: 0.8300\n",
      "    Model improved. Saved model to ./saved_models/model10_11.pth\n",
      "  Epoch 12/20, Loss: 0.0515\n",
      "    Validation Accuracy: 0.8067\n",
      "  Epoch 13/20, Loss: 0.0570\n",
      "    Validation Accuracy: 0.8200\n",
      "  Epoch 14/20, Loss: 0.0497\n",
      "    Validation Accuracy: 0.8033\n",
      "  Epoch 15/20, Loss: 0.0493\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 16/20, Loss: 0.0439\n",
      "    Validation Accuracy: 0.8233\n",
      "  Epoch 17/20, Loss: 0.0389\n",
      "    Validation Accuracy: 0.8167\n",
      "  Epoch 18/20, Loss: 0.0339\n",
      "    Validation Accuracy: 0.8333\n",
      "    Model improved. Saved model to ./saved_models/model10_18.pth\n",
      "  Epoch 19/20, Loss: 0.0435\n",
      "    Validation Accuracy: 0.8433\n",
      "    Model improved. Saved model to ./saved_models/model10_19.pth\n",
      "  Epoch 20/20, Loss: 0.0310\n",
      "    Validation Accuracy: 0.8400\n",
      "Finished training model 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    import zipfile\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader, Dataset, Subset\n",
    "    from torchvision import transforms, datasets\n",
    "    import timm\n",
    "    from PIL import Image\n",
    "\n",
    "    # ----------------------------\n",
    "    # Setup directories and transforms\n",
    "    # ----------------------------\n",
    "    data_dir = \"/content/dataset/data\"\n",
    "    train_dir = os.path.join(data_dir, \"train\")\n",
    "    val_dir = os.path.join(data_dir, \"val\")\n",
    "    test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "    # Define transforms (adjust as needed)\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                            [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                            [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Load training and validation data\n",
    "    # ----------------------------\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "    val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)\n",
    "\n",
    "    # Reverse the mapping so that index -> class label\n",
    "    idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "\n",
    "    # Create DataLoaders for validation (if needed)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Parameters and device\n",
    "    # ----------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = len(train_dataset.classes)  # should be 100\n",
    "    ensemble_size = 10     # Number of bagging models in the ensemble\n",
    "    num_epochs = 20       # Adjust epochs as needed\n",
    "    batch_size = 80\n",
    "\n",
    "    # ----------------------------\n",
    "    # Bagging Ensemble: Train multiple models on bootstrapped training sets\n",
    "    # ----------------------------\n",
    "    ensemble_models = []\n",
    "    for i in range(ensemble_size):\n",
    "        print(f\"Training model {i+1}/{ensemble_size}\")\n",
    "        # Create bootstrapped sample indices for training dataset\n",
    "        indices = np.random.choice(len(train_dataset), size=len(train_dataset), replace=True).tolist()\n",
    "        bagged_dataset = Subset(train_dataset, indices)\n",
    "        bagged_loader = DataLoader(bagged_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "        # Create model instance using seresnext101_64x4d with pretrained weights\n",
    "        model = timm.create_model('seresnext101_64x4d', pretrained=True, num_classes=num_classes)\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        best_acc = 0.0  # Initialize best accuracy for this model\n",
    "\n",
    "        # Training loop for the current ensemble model\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for images, labels in bagged_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            avg_loss = running_loss / len(bagged_loader)\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # ----------------------------\n",
    "            # Validation evaluation and model saving\n",
    "            # ----------------------------\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            val_acc = correct / total\n",
    "            print(f\"    Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            # Save the model if the current epoch's accuracy is the best so far\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                model_save_path = f\"./saved_models/model{i+1}_{epoch+1}.pth\"\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"    Model improved. Saved model to {model_save_path}\")\n",
    "            model.train()\n",
    "        ensemble_models.append(model)\n",
    "        print(f\"Finished training model {i+1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32466,
     "status": "ok",
     "timestamp": 1742988113230,
     "user": {
      "displayName": "Seanmamasde",
      "userId": "11889259840918057157"
     },
     "user_tz": -480
    },
    "id": "__8sQOhl8IEG",
    "outputId": "8e20230a-f262-4a14-c6ab-ebfbfa6a132b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference on test set using ensemble models...\n",
      "Saved predictions to prediction.csv\n",
      "Created zip file pred.zip\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Define a custom test dataset to include image names\n",
    "# ----------------------------\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_dir, transform=None):\n",
    "        self.test_dir = test_dir\n",
    "        self.transform = transform\n",
    "        self.images = [fname for fname in os.listdir(test_dir) if fname.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.test_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "\n",
    "test_dataset = TestDataset(test_dir, transform=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# ----------------------------\n",
    "# Inference on the test set using majority voting\n",
    "# ----------------------------\n",
    "print(\"Starting inference on test set using ensemble models...\")\n",
    "# Set all models to evaluation mode\n",
    "for model in ensemble_models:\n",
    "    model.eval()\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, names in test_loader:\n",
    "        images = images.to(device)\n",
    "        # Collect predictions from each ensemble member\n",
    "        ensemble_preds = []\n",
    "        for model in ensemble_models:\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            ensemble_preds.append(preds.cpu().numpy())\n",
    "\n",
    "        # Convert list of arrays to a 2D numpy array of shape (ensemble_size, batch_size)\n",
    "        ensemble_preds = np.array(ensemble_preds)\n",
    "        # Transpose to shape (batch_size, ensemble_size)\n",
    "        ensemble_preds = ensemble_preds.T\n",
    "\n",
    "        # For each image in the batch, apply majority voting\n",
    "        for name, preds in zip(names, ensemble_preds):\n",
    "            vote = Counter(preds).most_common(1)[0][0]\n",
    "            # Map numeric prediction back to the original class label using reversed mapping\n",
    "            label = idx_to_class[vote]\n",
    "            predictions.append((name.split(\".\")[0], label))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Save predictions: remove existing prediction.csv and pred.zip if they exist.\n",
    "# ----------------------------\n",
    "csv_filename = \"prediction.csv\"\n",
    "zip_filename = \"pred.zip\"\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    os.remove(csv_filename)\n",
    "if os.path.exists(zip_filename):\n",
    "    os.remove(zip_filename)\n",
    "\n",
    "# Save predictions to CSV with the required format: image_name,pred_label\n",
    "df = pd.DataFrame(predictions, columns=[\"image_name\", \"pred_label\"])\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Saved predictions to {csv_filename}\")\n",
    "\n",
    "# Create a zip file containing the prediction.csv file\n",
    "with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(csv_filename)\n",
    "print(f\"Created zip file {zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-Pl01_i8q5U"
   },
   "source": [
    "# Check param count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1742988113239,
     "user": {
      "displayName": "Seanmamasde",
      "userId": "11889259840918057157"
     },
     "user_tz": -480
    },
    "id": "-pxx6eTb219M",
    "outputId": "4f14b166-bf1d-41fc-fbf3-62d0fa3f74b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 86388884\n",
      "Trainable parameters: 86388884\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "38067e201cf542b19bc37710ca06cd16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f9d9cb7345d44c489baa79c86f41c43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9344cbef06d741d08908e4eafe63bc06",
       "IPY_MODEL_f9c075b8840a4f44bc02efe8f930f280",
       "IPY_MODEL_a6f3392bca084170abaf6074f56ef91c"
      ],
      "layout": "IPY_MODEL_eb7fa1fa3df7445f9062bfe3db66f235"
     }
    },
    "4433dfcdaa3f49d1979cbd73d32fdc50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4800c5ab958340f89159b366c355c842": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ce802315d9f4a0ebb6e92209a289c10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fb1886dcd634f2eaa8444789c9e934a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9344cbef06d741d08908e4eafe63bc06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6e3bcdf64a8456ba01bcd95b96aa3eb",
      "placeholder": "",
      "style": "IPY_MODEL_5ce802315d9f4a0ebb6e92209a289c10",
      "value": "model.safetensors:100%"
     }
    },
    "a6f3392bca084170abaf6074f56ef91c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4800c5ab958340f89159b366c355c842",
      "placeholder": "",
      "style": "IPY_MODEL_4433dfcdaa3f49d1979cbd73d32fdc50",
      "value": "335M/335M[00:01&lt;00:00,194MB/s]"
     }
    },
    "eb7fa1fa3df7445f9062bfe3db66f235": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6e3bcdf64a8456ba01bcd95b96aa3eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9c075b8840a4f44bc02efe8f930f280": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38067e201cf542b19bc37710ca06cd16",
      "max": 334892224,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7fb1886dcd634f2eaa8444789c9e934a",
      "value": 334892224
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
